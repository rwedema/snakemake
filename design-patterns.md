# Design patterns

## scatter/gather process

Use Snakemake’s explicit support for scatter/gather processes:

```python
#define n and sets n for each scatter/gather process in the workflow
scattergather: 
    someprocess=8 #default n = 8, can be overiden by cmd --set-scatter someprocess=16

rule scatter: 
"""split data into n items"""
    output:
        scatter.someprocess("scattered/{scatteritem}.txt")

rule step2: 
""" apply to each item"""
    input:
        "scattered/{scatteritem}.txt" 
    output:
        "transformed/{scatteritem}.txt"

rule gather: 
"""aggregate over the outputs of step2 for each item"""
    input:
        gather.someprocess("transformed/{scatteritem}.txt")
```



## streaming

Stream output between jobs, instead of writing it to disk:

```python
rule step1:
    output: 
        pipe("hello.txt")
    shell:
        "echo hello > {output}"

rule step2: 
    output:
        pipe("world.txt") 
    shell:
        "echo world > {output}"

rule step3: 
    input:
        "hello.txt",
        "world.txt" 
    output:
        "hello-world.txt" 
    shell:
        "cat {input} > {output}"
```

## non-file parameters

Data analysis steps can need additional non-file input in the form of parameters, that are for example obtained from the workflow configuration. The `params` keyword allows you to specify additional parameters depending on the wildcard values. In the example below, a lambda expression is defined, that retrieves a threshold depending on the value of the wildcard sample

```python
rule step: 
    input:
        "data/{sample}.txt" 
    output:
        "results/{sample}.txt" 
    params:
        threshold=lambda w: config["threshold"][w.sample] 
    shell:
        "some-tool -x {params.threshold} {input} > {output}"
```



## iteration

Sometimes, a certain step in a data analysis workflow needs to be applied iteratively. In `rule`` ``all` the iteration count is set to 10, in `rule`` ``iterate` it iterates from 0 to 10 with count variable `i` and the input is the iteration function `get_iteration_function`

```python
rule all: 
    input:
        "data.10.transformed.txt" #set to desired count 10
        
def get_iteration_input(wildcards):
    i = int(wildcards.i)
    if i == 0:
        #request initial data
        return "data.txt" 
    else:
        #request output of the previous iteration (i-1)
        return f"data.{i-1}.transformed.txt"

rule iterate: 
    input:
        get_iteration_input 
    output:
        #setting the itteration count variable i
        "data.{i}.transformed.txt" 
    
```

## sample sheet-based configuration

Snakemake workflows can be directly integrated with PEPs, Portable encapsulated projects (PEPs, https://pep.databio. org)

```python
pepfile: "pep/config.yaml" 
pepschema: "schemas/pep.yaml"

rule all: 
    input:
        expand("results/{sample}.someresult.txt", 
               sample=pep.sample_table["sample_name"])
```

## conditional execution

In the example below only the analysis path that shall be taken depends on the intermediate results of the quality control check. It uses the checkpoint object to filter out the samples that do not pass the quality control. In this example, the `checkpoint` rule `qc` creates a TSV file, which the function loads, to extract only those samples for which the column `"some-value"` contains a value greater than 90. Only for those samples, the file `"results/processed/{sample}.txt"` is requested, which is then generated by applying the rule process for each of these samples.

```python
def get_results(wildcards):
    # conduct the flow only on the samples that pass the Quality control (QC) 
    # by using a global checkpoints object 
    with checkpoints.qc.get().output[0].open() as f: 
        qc = pd.read_csv(f, sep="\t")
        return expand("results/processed/{sample}.txt", 
                  sample=qc[qc["some-value"] > 90.0]["sample"])
 
rule all: 
    # get the results of the 
    input:
        get_results
        
checkpoint qc: 
    input:
        expand("results/preprocessed/{sample}.txt", sample=samples) 
    output:
        "results/qc.tsv" 
    shell:
        "perfom-qc {input} > {output}"
        
rule process: 
    input:
        "results/preprocessed/{sample}.txt" 
    output:
        "results/processed/{sample}.txt" 
    shell:
        "process {input} > {output}"
```

## benchmarking

Upon execution of the job from the rule where the benchmark directive is used, Snakemake will constantly measure CPU and memory consumption, and store averaged results together with runtime information in the given file:

```python
rule step:
    input:
        "data/{sample}.txt"
    output:
        "results/{sample}.txt"
    benchmark:
        "benchmarks/some-tool/{sample}.txt  
    shell:
        "some-tool {input} > {output}"
```

## parameter space exploration

Paramspace wraps a Pandas data frame allowing retrieval of a wildcard pattern (via the property `wildcard_pattern`) that encodes each column of the data frame in the form name\~{name} (column name followed by the wildcard/wildcard value). By this, we can iterate over all the columns and values.&#x20;

```python
from snakemake.utils import Paramspace
import pandas as pd

paramspace = Paramspace(pd.read_csv("params.tsv", sep="\t")) 

rule all:
    input:
        expand("results/simulations/{params}.pdf",
                params=paramspace.instance_patterns)

rule simulate:
    output:
        f"results/simulations/{paramspace.wildcard_pattern}.tsv" 
    params:
        simulation=paramspace.instance
```

source: Mölder F, Jablonski KP, Letcher B et al.. Sustainable data analysis with Snakemake \[version 1]. F1000Research 2021, 10:33 (doi: 10.12688/f1000research.29032.1)
